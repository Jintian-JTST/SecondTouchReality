# SecondTouchReality
This is the second edition for OneTouchReality. Namely after the project in Adventure X 2025, and aiming for Cambridge EduX Hackathon 2025. This is a cross-platform mini system: Python uses MediaPipe to detect hand landmarks and estimate depth → sends the data via UDP to Unity → Unity objects trigger actions and send commands back to Python via TCP → Python relays them to Arduino over serial.

---

## Overview

## Project Structure

## Requirements

### Python

## Hand Depth Estimation Algorithm

The hand depth estimation algorithm uses geometric features from MediaPipe Hands landmarks to compute two independent depth channels (based on palm width and palm length), then fuses them using gesture-aware weights derived from hand pose states (curl, side, palm front). Temporal filtering is applied for smooth output. Here’s a step-by-step breakdown:

1. **Input & Detection**
   Grab frames from the camera; run MediaPipe Hands to get 21 hand landmarks (3D normalized coords).

2. **Geometric Features**

   * Compute palm width `palm_width` (pixel distance between index MCP and pinky MCP).
   * Compute palm length `palm_length` (path length 0→9→10→11→12).

3. **Hand Pose States**

   * Curl `curl`: ratio of straight-line (wrist→middle fingertip) to finger path length, mapped to [0,1].
   * Side `side`: side-rotation score from deviation of current width/length ratio vs calibrated ratio.
   * Palm front `palm_front`: palm-vs-back score from the plane normal of points 0/5/17 versus camera direction.

4. **Calibration & Dual Depth Channels**

   * During calibration, record median width/length and true distance to derive constants `k_w, k_l`.
   * At runtime, compute two depth channels: `Zw = k_w / palm_width`, `Zl = k_l / palm_length`.

5. **Gesture-Aware Fusion**

   * Use `curl`, `side`, and `palm_front` to compute weights `w_w, w_l`:
     front + fist → trust `Zw` more; side-view → trust `Zl` more.
   * Fuse geometry: `Z_mix = w_w * Zw + w_l * Zl`.
   * Apply fist-shortening correction in the “front + palm + curled” regime to get `Z_final`.

6. **Temporal Filtering & Output**

   * Apply median filter + EMA over `Z_final` across frames to obtain smooth depth `Z_disp`.
   * Show `curl / side / weights / Z_disp` in the HUD and use `Z_disp` as the final depth output.

































# The following items are completely generated by ChatGPT according to my prompt and goals and details. They should be in the status of TBD.




---


## Quick Start

### 1 Start the TCP→Serial bridge

1. Open `tcp_to_serial_bridge.py` and adjust:

   ```python
   SERIAL_PORT = 'COM5'
   BAUD_RATE = 9600
   SERVER_HOST = "127.0.0.1"
   SERVER_PORT = 8000
   ```

   Run the script; it opens the serial port and listens for TCP clients.
2. The bridge expects messages like `finger,angle\n` and writes them as `finger angle\n`.
   Example: `index,120\n` → `index 120\n`.

> The handler only forwards lines containing a comma split into two fields.

### 2 Run the hand-tracking script

1. Run `hand_two_hands_z_udp.py`. It opens your webcam and shows both hands; the HUD displays the wrist (`id=0`) depth values and FPS.
2. Press **c** to calibrate: place your wrist at a known distance (in meters), enter that number in the console. The script computes `scale = real_d / z_vis`, then reports distances in meters (`z_m = z_vis * scale`).
3. Each frame’s data is sent as JSON via UDP to `127.0.0.1:5065`.

### 3 Open Unity and connect

* **Dragging demo:**
  Attach `DraggableObjectController` to a `Rigidbody` object. Adjust `dragSpeed`, `maxVelocity`, etc. The script uses raycasts and screen-to-world conversions for smooth physics dragging.
* **Trigger sender:**
  Attach `CollisionAndColorChanger` to a trigger object. Set `pythonServerIp` and `pythonServerPort` to the bridge. When another object enters, it turns red and sends a random angle string via TCP; on exit, it reverts color.

> Note: The provided Unity script currently sends **only one number (angle)**, while the bridge expects `finger,angle`. Either modify Unity to send `"finger,angle\n"`, or update the bridge to interpret single-number messages.

---

## Data Protocols

### UDP (Python → Unity)

* Target: `127.0.0.1:5065`.
* Example JSON frame:

  ```json
  {
    "timestamp": 1731780000.123,
    "fps": 30.2,
    "hands": [
      {
        "hand_index": 0,
        "landmarks": [
          {
            "id": 0,
            "normalized": {"x": 0.51, "y": 0.42, "z": -0.12},
            "pixel": {"x": 326, "y": 202},
            "z_rel": -0.12,
            "z_vis": 0.12,
            "z_m": 0.38
          }
        ]
      }
    ]
  }
  ```

  **Fields:**

  * `normalized`: MediaPipe’s normalized coordinates (`z` negative toward camera).
  * `z_vis`: Flipped sign (`-z_rel`) for “larger = closer.”
  * `z_m`: Real-world distance after calibration.
  * Wrist landmark is `id=0`.

### TCP (Unity → Python → Arduino)

* Recommended message: `finger,angle\n` (e.g., `index,120\n`). The bridge writes it to serial as `index 120\n`.
* The included Unity script sends only a single integer angle (0–180). Adapt it for full multi-servo use.

---

## Calibration (Z to Meters)

1. Run the Python hand-tracking script.
2. Press **c**, hold your wrist at a known distance (e.g., `0.40` m).
3. Enter that distance. The script calculates `scale = D / z_vis` and displays it on HUD; all following frames use this to compute `z_m`.

> If `z_vis <= 0`, calibration fails; reposition and retry.

---

## Unity Components

### DraggableObjectController

* Requires a `Rigidbody`. Uses raycasts and `ScreenToWorldPoint` to set a target position and moves the object with velocity capped at `maxVelocity`.
* Gravity is disabled while dragging; re-enable it manually after release if needed.

### CollisionAndColorChanger

* Connects to the Python bridge (`localhost:8000` by default) on `Start()`.
* `OnTriggerEnter`: changes color to red and sends a random angle (0–180).
* `OnTriggerExit`: restores original color.
* `targetTag` is present but unused—extend as needed.

---

## Arduino Sketch Concept

* Reads lines over serial at `9600` baud.
* Parses commands like `finger angle` and maps them to servo pins and angles.
* Ensure common ground, proper voltage, and servo angle limits.

---

## Troubleshooting

* **UDP/TCP not connecting:** Check firewall and port numbers; ensure both Unity and Python configs match.
* **Serial open error:** Verify `COM` port and exclusivity—only one process can use it.
* **No hand detected:** Ensure the camera isn’t busy and lighting is good (default 640×480).
* **Calibration incorrect:** Re-measure distance accurately; ensure wrist faces camera.
* **Protocol mismatch:** Unity may send only angles, while bridge expects `finger,angle`; unify the format or adjust parsing.

---

## License

No explicit license is included; add your own LICENSE before publishing or redistributing.

---

## Future Improvements

* Parse UDP JSON inside Unity to drive 3D hand visualization or interaction depth.
* Unify command protocol with a GUI for per-finger servo mapping.
* Add safe speed interpolation and calibration workflow on Arduino side.
